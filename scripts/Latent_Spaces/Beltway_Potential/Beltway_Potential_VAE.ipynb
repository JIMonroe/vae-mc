{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate \"Washington Beltway\" potential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data according to a \"Washington Beltway\" potential\n",
    "#Or sort of like that... just two seperate potential energy minima along the radial distance and uniform in angle\n",
    "offset = 0.0 #Offset between energy wells in radial distance, in kB*T\n",
    "mix = 1.0 / (1.0 + np.exp(offset))\n",
    "print(mix)\n",
    "r_dist = tfp.distributions.Mixture(tfp.distributions.Categorical(probs=[mix, 1.0-mix]),\n",
    "                                   [tfp.distributions.Gamma(40.0, rate=40.0),\n",
    "                                    tfp.distributions.Gamma(160.0, rate=80.0)])\n",
    "theta_dist = tfp.distributions.Uniform(low=-np.pi, high=np.pi)\n",
    "wb_dist = tfp.distributions.JointDistributionSequential([r_dist, theta_dist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbins = np.linspace(0.0, 3.0, 100)\n",
    "plt.hist(r_dist.sample(100000).numpy().flatten(), bins=rbins, histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetabins = np.linspace(-np.pi, np.pi, 100)\n",
    "plt.hist(theta_dist.sample(100000).numpy().flatten(), bins=thetabins, histtype='step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_sample = tf.stack(wb_dist.sample(100000), axis=1).numpy()\n",
    "plt.hist2d(wb_sample[:, 0], wb_sample[:, 1], bins=[rbins, thetabins])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = wb_sample[:, 0]*np.cos(wb_sample[:, 1])\n",
    "y_sample = wb_sample[:, 0]*np.sin(wb_sample[:, 1])\n",
    "wb_xy_sample = np.vstack([x_sample, y_sample]).T\n",
    "\n",
    "xybins = np.linspace(-3.0, 3.0, 100)\n",
    "\n",
    "plt.hist2d(wb_xy_sample[:, 0], wb_xy_sample[:, 1], bins=[xybins, xybins])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('wb_rtheta_data', wb_sample)\n",
    "np.save('wb_xy_data', wb_xy_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_xy = np.load('wb_xy_data.npy')\n",
    "dat_rtheta = np.load('wb_rtheta_data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "from libVAE import vae, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define VAE parameters\n",
    "params_vae = {'e_hidden_dim':50,\n",
    "              'f_hidden_dim':20,\n",
    "              'd_hidden_dim':50}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training and plotting routines\n",
    "\n",
    "def train_vae(model, raw_data,\n",
    "              num_epochs=100, batch_size=200, anneal_beta_val=None):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, #0.005\n",
    "                                         beta_1=0.9,\n",
    "                                         beta_2=0.999,\n",
    "                                         epsilon=1e-08)\n",
    "    \n",
    "    if model.autoregress:\n",
    "        loss_fn = losses.AutoregressiveLoss(model.decoder,\n",
    "                                            reduction=tf.keras.losses.Reduction.SUM)\n",
    "    else:\n",
    "        loss_fn = losses.ReconLoss(loss_fn=losses.diag_gaussian_loss, activation=None,\n",
    "                                   reduction=tf.keras.losses.Reduction.SUM)\n",
    "    \n",
    "    train_data = tf.data.Dataset.from_tensor_slices(raw_data)\n",
    "    train_data = train_data.shuffle(buffer_size=3*batch_size).batch(batch_size)\n",
    "    \n",
    "    if anneal_beta_val is not None:\n",
    "        try:\n",
    "            original_beta = copy.deepcopy(model.beta)\n",
    "        except AttributeError:\n",
    "            print(\"Annealing turned on but model has no beta parameter, turning off.\")\n",
    "            anneal_beta_val = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        if anneal_beta_val is not None:\n",
    "            model.beta = (anneal_beta_val - original_beta)*epoch/(num_epochs - 1)\n",
    "        \n",
    "        print(\"Epoch %i (beta=%f):\"%(epoch, model.beta))\n",
    "        \n",
    "        for step, batch_train in enumerate(train_data):\n",
    "                        \n",
    "            for ametric in model.metrics:\n",
    "                ametric.reset_states()\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                reconstructed = model(batch_train, training=True)\n",
    "                loss = loss_fn(batch_train, reconstructed) / batch_train.shape[0]\n",
    "                loss += sum(model.losses)\n",
    "                \n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            \n",
    "            if step%100 == 0:\n",
    "                print('\\tStep %i: loss=%f, model_loss=%f, kl_div=%f, reg_loss=%f'\n",
    "                      %(step, loss, sum(model.losses), model.metrics[0].result(), model.metrics[1].result()))\n",
    "            \n",
    "            if tf.math.is_nan(loss):\n",
    "                raise ValueError('Loss is NaN.')\n",
    "                \n",
    "            gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train in 1D\n",
    "Train with x and y coordinates, then r and theta, with prior flow VAE, prior flow with regular autoregression, then full flow VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First with x and y data for all models\n",
    "vae_xy_1D = vae.PriorFlowVAE((2,), 1, include_vars=True, autoregress=False, **params_vae)\n",
    "vae_xy_1D_auto = vae.PriorFlowVAE((2,), 1, include_vars=True, autoregress=True, **params_vae)\n",
    "vae_xy_1D_full = vae.FullFlowVAE((2,), 1, **params_vae)\n",
    "\n",
    "for model, fname in [[vae_xy_1D, 'vae_xy_1D'],\n",
    "                     [vae_xy_1D_auto, 'vae_xy_1D_auto'],\n",
    "                     [vae_xy_1D_full, 'vae_xy_1D_full']]:\n",
    "    model.beta = 0.0\n",
    "    train_vae(model, dat_xy, num_epochs=20, anneal_beta_val=1.0)\n",
    "    train_vae(model, dat_xy, num_epochs=80)\n",
    "    model.save_weights(fname+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next with r and theta data for all models\n",
    "#Make sure to specify that theta is periodic (will use -pi to pi)\n",
    "#Unfortunately, no way to set bounds for r from here...\n",
    "#But default of -10 to 10 should be alright because contains range for r\n",
    "vae_rtheta_1D = vae.PriorFlowVAE((2,), 1, include_vars=True, autoregress=False, periodic_dof_inds=[1], **params_vae)\n",
    "vae_rtheta_1D_auto = vae.PriorFlowVAE((2,), 1, include_vars=True, autoregress=True, periodic_dof_inds=[1], **params_vae)\n",
    "vae_rtheta_1D_full = vae.FullFlowVAE((2,), 1, periodic_dof_inds=[1], **params_vae)\n",
    "\n",
    "for model, fname in [[vae_rtheta_1D, 'vae_rtheta_1D'],\n",
    "                     [vae_rtheta_1D_auto, 'vae_rtheta_1D_auto'],\n",
    "                     [vae_rtheta_1D_full, 'vae_rtheta_1D_full']]:\n",
    "    model.beta = 0.0\n",
    "    train_vae(model, dat_rtheta, num_epochs=20, anneal_beta_val=1.0)\n",
    "    train_vae(model, dat_rtheta, num_epochs=80)\n",
    "    model.save_weights(fname+'.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train in 2D\n",
    "Train with x and y coordinates, then r and theta, with prior flow VAE, prior flow with regular autoregression, then full flow VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First with x and y data for all models\n",
    "vae_xy_2D = vae.PriorFlowVAE((2,), 2, include_vars=True, autoregress=False, **params_vae)\n",
    "vae_xy_2D_auto = vae.PriorFlowVAE((2,), 2, include_vars=True, autoregress=True, **params_vae)\n",
    "vae_xy_2D_full = vae.FullFlowVAE((2,), 2, **params_vae)\n",
    "\n",
    "for model, fname in [[vae_xy_2D, 'vae_xy_2D'],\n",
    "                     [vae_xy_2D_auto, 'vae_xy_2D_auto'],\n",
    "                     [vae_xy_2D_full, 'vae_xy_2D_full']]:\n",
    "    model.beta = 0.0\n",
    "    train_vae(model, dat_xy, num_epochs=20, anneal_beta_val=1.0)\n",
    "    train_vae(model, dat_xy, num_epochs=80)\n",
    "    model.save_weights(fname+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next with r and theta data for all models\n",
    "vae_rtheta_2D = vae.PriorFlowVAE((2,), 2, include_vars=True, autoregress=False, periodic_dof_inds=[1], **params_vae)\n",
    "vae_rtheta_2D_auto = vae.PriorFlowVAE((2,), 2, include_vars=True, autoregress=True, periodic_dof_inds=[1], **params_vae)\n",
    "vae_rtheta_2D_full = vae.FullFlowVAE((2,), 2, periodic_dof_inds=[1], **params_vae)\n",
    "\n",
    "for model, fname in [[vae_rtheta_2D, 'vae_rtheta_2D'],\n",
    "                     [vae_rtheta_2D_auto, 'vae_rtheta_2D_auto'],\n",
    "                     [vae_rtheta_2D_full, 'vae_rtheta_2D_full']]:\n",
    "    model.beta = 0.0\n",
    "    train_vae(model, dat_rtheta, num_epochs=20, anneal_beta_val=1.0)\n",
    "    train_vae(model, dat_rtheta, num_epochs=80)\n",
    "    model.save_weights(fname+'.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

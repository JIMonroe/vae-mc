# Written in 2020 by Jacob I. Monroe, NIST Employee

"""Defines methods to load data for training VAE models."""

import numpy as np
import tensorflow as tf
from netCDF4 import Dataset

def raw_image_data(datafile):
  """Reads in data in netcdf format and retuns numpy array.
Really working with lattice gas snapshots, which we can consider images.
If given a list, will loop over."""
  if isinstance(datafile, str):
    dat = Dataset(datafile, 'r')
    images = np.array(dat['config'][:,:,:], dtype='float32')
    dat.close()
  else:
    images = np.array([])
    for i, f in enumerate(datafile):
      dat = Dataset(f, 'r')
      if i == 0:
        images = np.array(dat['config'][:,:,:], dtype='float32')
      else:
        images = np.vstack((images, np.array(dat['config'][:,:,:], dtype='float32')))
      dat.close()
  #Most images have 3 dimensions, so to fit with previous VAE code for images, add dimension
  images = np.reshape(images, images.shape+(1,))
  #And want to shuffle data randomly
  np.random.shuffle(images)
  return images


def image_data(datafile, batch_size, val_frac=0.1):
  """Takes raw data as numpy array and converts to training and validation tensorflow datasets."""
  rawData = raw_image_data(datafile)
  #Save some fraction of data for validation
  valInd = int((1.0-val_frac)*rawData.shape[0])
  trainData = tf.data.Dataset.from_tensor_slices(rawData[:valInd])
  trainData = trainData.shuffle(buffer_size=3*batch_size).batch(batch_size)
  trainData = tf.data.Dataset.zip((trainData, trainData))
  valData = tf.data.Dataset.from_tensor_slices(rawData[valInd:])
  valData = valData.batch(batch_size)
  valData = tf.data.Dataset.zip((valData, valData))
  return trainData, valData


def dsprites_data(batch_size, val_frac=0.01):
  """Loads in the dsprites dataset from tensorflow_datasets.
  """
  import tensorflow_datasets as tfds

  def imFromDict(adict):
    return tf.cast(adict['image'], 'float32')

  valPercent = int(val_frac*100)
  trainData = tfds.load("dsprites", split="train[:%i%%]"%(100-valPercent))
  trainData = trainData.map(imFromDict)
  trainData = trainData.shuffle(buffer_size=batch_size).batch(batch_size, drop_remainder=True)
  trainData = tf.data.Dataset.zip((trainData, trainData))
  valData = tfds.load("dsprites", split="train[-%i%%:]"%(valPercent))
  valData = valData.map(imFromDict)
  valData = valData.batch(batch_size, drop_remainder=True)
  valData = tf.data.Dataset.zip((valData, valData))
  return trainData, valData


def dimer_2D_data(datafile, batch_size, val_frac=0.01,
                  dset='all', permute=True, center_and_whiten=False):
  """Loads data from a system of 2D LJ particles with one special dimer
generated by Frank Noe's deep_boltzmann package.
  """
  trajdict = np.load(datafile, allow_pickle=True)

  #Load open and closed states or just one of them
  if dset == 'all':
    trajStrs = ['traj_open', 'traj_closed']
  else:
    trajStrs = ['traj_%s'%dset]

  #Use permuted particle labelling if desired
  if permute:
    trajStrs = [s+'_hungarian' for s in trajStrs]

  rawData = np.vstack([trajdict[s] for s in trajStrs])
  #And want to shuffle data randomly (just a good idea)
  np.random.shuffle(rawData)

  #Center and whiten the data if set to
  #If train model with this, make sure to unwhiten before comparing
  if center_and_whiten:
    rawData -= np.mean(rawData, axis=0)
    rawData /= np.std(rawData, axis=0, ddof=1)

  #Re-order particles in a way that will work well with autoregression
  #Remember, data is x1, y1, x2, y2, etc.
  #Spiralling out from dimer
  particle_order = [0, 1, #Dimer particles
               17, 16, 22, 23, #4 particles closest to dimer
               18, 11, 10, 15, 21, 28, 29, 24, #Next layer around dimer excluding corners
               12, 9, 27, 30, #Corners, which have less freedom
               31, 25, 19, 13, 6, 5, 4, 3, 8, 14, 20, 26, 33, 34, 35, 36, #Outer layer, no corners
               37, 7, 2, 32] #Outermost corners
  new_order = np.zeros(rawData.shape[1])
  new_order[::2] = np.array(particle_order)*2
  new_order[1::2] = np.array(particle_order)*2 + 1
  rawData = rawData[:, new_order]

  #Save some fraction of data for validation
  valInd = int((1.0-val_frac)*rawData.shape[0])
  trainData = tf.data.Dataset.from_tensor_slices(rawData[:valInd])
  trainData = trainData.shuffle(buffer_size=3*batch_size).batch(batch_size)
  trainData = tf.data.Dataset.zip((trainData, trainData))
  valData = tf.data.Dataset.from_tensor_slices(rawData[valInd:])
  valData = valData.batch(batch_size)
  valData = tf.data.Dataset.zip((valData, valData))
  return trainData, valData



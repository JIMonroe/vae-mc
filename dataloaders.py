# Written in 2020 by Jacob I. Monroe, NIST Employee

"""Defines methods to load data for training VAE models."""

import numpy as np
import tensorflow as tf
from netCDF4 import Dataset

def raw_image_data(datafile):
  """Reads in data in netcdf format and retuns numpy array.
Really working with lattice gas snapshots, which we can consider images.
If given a list, will loop over."""
  if isinstance(datafile, str):
    dat = Dataset(datafile, 'r')
    images = np.array(dat['config'][:,:,:], dtype='float32')
    dat.close()
  else:
    images = np.array([])
    for i, f in enumerate(datafile):
      dat = Dataset(f, 'r')
      if i == 0:
        images = np.array(dat['config'][:,:,:], dtype='float32')
      else:
        images = np.vstack((images, np.array(dat['config'][:,:,:], dtype='float32')))
      dat.close()
  #Most images have 3 dimensions, so to fit with previous VAE code for images, add dimension
  images = np.reshape(images, images.shape+(1,))
  #And want to shuffle data randomly
  np.random.shuffle(images)
  return images


def image_data(datafile, batch_size, val_frac=0.1):
  """Takes raw data as numpy array and converts to training and validation tensorflow datasets."""
  rawData = raw_image_data(datafile)
  #Save some fraction of data for validation
  valInd = int((1.0-val_frac)*rawData.shape[0])
  trainData = tf.data.Dataset.from_tensor_slices(rawData[:valInd])
  trainData = trainData.shuffle(buffer_size=3*batch_size).batch(batch_size)
  trainData = tf.data.Dataset.zip((trainData, trainData))
  valData = tf.data.Dataset.from_tensor_slices(rawData[valInd:])
  valData = valData.batch(batch_size)
  valData = tf.data.Dataset.zip((valData, valData))
  return trainData, valData


def dsprites_data(batch_size, val_frac=0.01):
  """Loads in the dsprites dataset from tensorflow_datasets.
  """
  import tensorflow_datasets as tfds

  def imFromDict(adict):
    return tf.cast(adict['image'], 'float32')

  valPercent = int(val_frac*100)
  trainData = tfds.load("dsprites", split="train[:%i%%]"%(100-valPercent))
  trainData = trainData.map(imFromDict)
  trainData = trainData.shuffle(buffer_size=batch_size).batch(batch_size, drop_remainder=True)
  trainData = tf.data.Dataset.zip((trainData, trainData))
  valData = tfds.load("dsprites", split="train[-%i%%:]"%(valPercent))
  valData = valData.map(imFromDict)
  valData = valData.batch(batch_size, drop_remainder=True)
  valData = tf.data.Dataset.zip((valData, valData))
  return trainData, valData


def dimer_2D_data(datafile, batch_size, val_frac=0.01,
                  dset='all', permute=True, center_and_whiten=False):
  """Loads data from a system of 2D LJ particles with one special dimer
generated by Frank Noe's deep_boltzmann package.
  """
  trajdict = np.load(datafile, allow_pickle=True)

  #Load open and closed states or just one of them
  if dset == 'all':
    trajStrs = ['traj_open', 'traj_closed']
  else:
    trajStrs = ['traj_%s'%dset]

  #Use permuted particle labelling if desired
  if permute:
    trajStrs = [s+'_hungarian' for s in trajStrs]

  rawData = np.vstack([trajdict[s] for s in trajStrs])
  #And want to shuffle data randomly (just a good idea)
  np.random.shuffle(rawData)

  #Center and whiten the data if set to
  #If train model with this, make sure to unwhiten before comparing
  if center_and_whiten:
    rawData -= np.mean(rawData, axis=0)
    rawData /= np.std(rawData, axis=0, ddof=1)

  #Re-order particles in a way that will work well with autoregression
  #Remember, data is x1, y1, x2, y2, etc.
  #Spiralling out from dimer
  particle_order = [0, 1, #Dimer particles
               17, 16, 22, 23, #4 particles closest to dimer
               18, 11, 10, 15, 21, 28, 29, 24, #Next layer around dimer excluding corners
               12, 9, 27, 30, #Corners, which have less freedom
               31, 25, 19, 13, 6, 5, 4, 3, 8, 14, 20, 26, 33, 34, 35, 36, #Outer layer, no corners
               37, 7, 2, 32] #Outermost corners
  new_order = np.zeros(rawData.shape[1], dtype=int)
  new_order[::2] = np.array(particle_order)*2
  new_order[1::2] = np.array(particle_order)*2 + 1
  rawData = rawData[:, new_order]

  #Save some fraction of data for validation
  valInd = int((1.0-val_frac)*rawData.shape[0])
  trainData = tf.data.Dataset.from_tensor_slices(rawData[:valInd])
  trainData = trainData.shuffle(buffer_size=3*batch_size).batch(batch_size)
  trainData = tf.data.Dataset.zip((trainData, trainData))
  valData = tf.data.Dataset.from_tensor_slices(rawData[valInd:])
  valData = valData.batch(batch_size)
  valData = tf.data.Dataset.zip((valData, valData))
  return trainData, valData


def ala_dipeptide_data(datafile, batch_size, val_frac=0.01):
  """Loads data for alanine dipeptide. Can load either XYZ or BAT coordinates, but in
analysis, make sure to also use MDAnalysis.analysis.bat to switch between them if needed.
  """
  rawData = np.load(datafile).astype('float32')
  #Save some fraction of data for validation
  valInd = int((1.0-val_frac)*rawData.shape[0])
  trainData = tf.data.Dataset.from_tensor_slices(rawData[:valInd])
  trainData = trainData.shuffle(buffer_size=3*batch_size).batch(batch_size)
  trainData = tf.data.Dataset.zip((trainData, trainData))
  valData = tf.data.Dataset.from_tensor_slices(rawData[valInd:])
  valData = valData.batch(batch_size)
  valData = tf.data.Dataset.zip((valData, valData))
  return trainData, valData


def polymer_data(datafile, batch_size, val_frac=0.01, rigid_bonds=False):
  """Loads data for alanine dipeptide. Can load either XYZ or BAT coordinates, but in
analysis, make sure to also use MDAnalysis.analysis.bat to switch between them if needed.
If have rigid bonds, best to incorporate these constraints into model, which is naturally
accomplished with BAT coordinates and only passing DOFs that aren't bonds.
  """
  rawData = np.load(datafile).astype('float32')

  #If have rigid bonds (and have BAT coordinates so can constrain them for VAE model)
  #then want to identify them and mask them out in the training data
  #First 3 DOFs are root atom coordinates, next three are rotational coordinates
  #Next 2 are first two bonds, then we have the first angle
  if 'BAT' in datafile and rigid_bonds:

    bond_inds = list(range(8)) #[6, 7] Masking rigid translation and rotation as well
    #Next have all bonds, with this number depending on number of atoms
    #For polymer, should just have one bead per monomer, so Nbonds = N-1
    for b in range(9, 9 + rawData.shape[1]//3 - 3):
      bond_inds.append(b)
    bond_mask = np.ones(rawData.shape[1], dtype=bool)
    bond_mask[bond_inds] = False
    rawData = rawData[:, bond_mask]

    #Also reorder so that autoregressive model predicts angle, dihedral, angle dihedral, etc.
    dof_order = []
    for a in range(rawData.shape[1]//3 - 2): #Number of angles
      dof_order.append(a)
      #Only add dihedral if have any left
      if a < rawData.shape[1]//3 - 3:
        dof_order.append(a + rawData.shape[1]//3 - 2)
    rawData = rawData[:, dof_order]

  #If have no cyclic structures, above should work for any bond topography
  #Can also consider masking out root atom translation and rigid rotation DOFs
  #Save some fraction of data for validation
  valInd = int((1.0-val_frac)*rawData.shape[0])
  trainData = tf.data.Dataset.from_tensor_slices(rawData[:valInd])
  trainData = trainData.shuffle(buffer_size=3*batch_size).batch(batch_size)
  trainData = tf.data.Dataset.zip((trainData, trainData))
  valData = tf.data.Dataset.from_tensor_slices(rawData[valInd:])
  valData = valData.batch(batch_size)
  valData = tf.data.Dataset.zip((valData, valData))
  return trainData, valData


